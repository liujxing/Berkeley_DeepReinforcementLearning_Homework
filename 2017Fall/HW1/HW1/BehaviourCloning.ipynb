{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to load an expert policy and generate roll-out data for behavioral cloning.\n",
    "Example usage:\n",
    "    python run_expert.py experts/Humanoid-v1.pkl Humanoid-v1 --render \\\n",
    "            --num_rollouts 20\n",
    "\n",
    "Author of this script and included expert policies: Jonathan Ho (hoj@openai.com)\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_util\n",
    "import gym\n",
    "import load_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('expert_policy_file', type=str)\n",
    "parser.add_argument('envname', type=str)\n",
    "parser.add_argument('--render', action='store_true')\n",
    "parser.add_argument(\"--max_timesteps\", type=int)\n",
    "parser.add_argument('--num_rollouts', type=int, default=20,\n",
    "                    help='Number of expert roll outs')\n",
    "#args = parser.parse_args([\"experts/Humanoid-v1.pkl\", \"Humanoid-v1\", \"--render\", \"--num_rollouts\", \"200\"])\n",
    "args = parser.parse_args([\"experts/Humanoid-v1.pkl\", \"Humanoid-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Reacher-v1.pkl\", \"Reacher-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Reacher-v1.pkl\", \"Reacher-v1\", \"--num_rollouts\", \"20000\"])\n",
    "#args = parser.parse_args([\"experts/Hopper-v1.pkl\", \"Hopper-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Hopper-v1.pkl\", \"Hopper-v1\", \"--num_rollouts\", \"200\"])\n",
    "#args = parser.parse_args([\"experts/Walker2d-v1.pkl\", \"Walker2d-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Walker2d-v1.pkl\", \"Walker2d-v1\", \"--num_rollouts\", \"200\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "('obs', (1, 376), (1, 376))\n"
     ]
    }
   ],
   "source": [
    "print('loading and building expert policy')\n",
    "# policy_fn is a function that takes an observation and outputs an action\n",
    "policy_fn = load_policy.load_policy(args.expert_policy_file)\n",
    "# The printed value is the shape of observation mean and observation std, to normalize observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-26 21:26:38,162] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The observation space for this environment:', Box(376,))\n",
      "('The action space for this environment:', Box(17,))\n",
      "('iter', 0)\n",
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-09b6f3427e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# done: boolean, whether it's time to reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# _, info: dict, diagnostic information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the simulator perform the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mtotalr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/envs/mujoco/humanoid.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpos_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmass_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpos_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmass_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/envs/mujoco/humanoid.pyc\u001b[0m in \u001b[0;36mmass_center\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_mass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mxpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxipos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmass\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHumanoidEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmujoco_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMujocoEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEzPickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1848\u001b[0;31m                             out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/numpy/core/_methods.pyc\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('loaded and built')\n",
    "\n",
    "with tf.Session():\n",
    "    tf_util.initialize()\n",
    "\n",
    "    import gym\n",
    "    env = gym.make(args.envname) # generate the environment\n",
    "    # This indicates the observation space, i.e. the universe in this environment, is indicated by this 376 dimension real rector\n",
    "    print(\"The observation space for this environment:\", env.observation_space)\n",
    "    # This indicates the action space, i.e. the action in this environment. Here the action is indicated by 17 dimension real vector.\n",
    "    print(\"The action space for this environment:\", env.action_space)\n",
    "    # use args.max_timesteps if it is not None, else use envs.spec.timestep_limit, here is envs.spec.timestep_limit = 1000\n",
    "    max_steps = args.max_timesteps or env.spec.timestep_limit \n",
    "\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    for i in range(args.num_rollouts):\n",
    "        print('iter', i)\n",
    "        obs = env.reset() # obs is not the initial observation of the environment\n",
    "        done = False\n",
    "        totalr = 0. # total reward\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            # the obs[None,:] is equivalent to obs.reshape(1,-1)\n",
    "            action = policy_fn(obs[None,:]) # generate the action from the current observation\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            # obs: object, an environment-specific object representing the observation of the environment\n",
    "            # r: float, reward obtained from action\n",
    "            # done: boolean, whether it's time to reset the environment\n",
    "            # _, info: dict, diagnostic information\n",
    "            obs, r, done, _ = env.step(action) # the simulator perform the action\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)\n",
    "\n",
    "    print('returns', returns) # the sum of rewards acorss each rollout\n",
    "    print('mean return', np.mean(returns)) # the mean across each rollout\n",
    "    print('std of return', np.std(returns)) # the std across each rollout\n",
    "\n",
    "    expert_data = {'observations': np.array(observations), # obsevations array has shape rollout * max_steps, each observation has length 376 as dimension of world\n",
    "                   'actions': np.array(actions)} # \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width_dense = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(width_dense, input_shape=env.observation_space.shape, activation=\"relu\"))\n",
    "model.add(Dense(width_dense, activation=\"relu\"))\n",
    "model.add(Dense(np.prod(env.action_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0894    \n",
      "Epoch 2/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0224    \n",
      "Epoch 3/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0142    \n",
      "Epoch 4/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0111    \n",
      "Epoch 5/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0094    \n",
      "Epoch 6/50\n",
      "200000/200000 [==============================] - 25s - loss: 0.0084    \n",
      "Epoch 7/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0076    \n",
      "Epoch 8/50\n",
      "200000/200000 [==============================] - 20s - loss: 0.0071    \n",
      "Epoch 9/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0067    \n",
      "Epoch 10/50\n",
      "200000/200000 [==============================] - 20s - loss: 0.0063    \n",
      "Epoch 11/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0060    \n",
      "Epoch 12/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0058    \n",
      "Epoch 13/50\n",
      "200000/200000 [==============================] - 20s - loss: 0.0056    \n",
      "Epoch 14/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0053    \n",
      "Epoch 15/50\n",
      "200000/200000 [==============================] - 25s - loss: 0.0052    \n",
      "Epoch 16/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0050    \n",
      "Epoch 17/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0049    \n",
      "Epoch 18/50\n",
      "200000/200000 [==============================] - 25s - loss: 0.0048    \n",
      "Epoch 19/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0046    \n",
      "Epoch 20/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0045    \n",
      "Epoch 21/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0044    \n",
      "Epoch 22/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0044    \n",
      "Epoch 23/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0043    \n",
      "Epoch 24/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0042    \n",
      "Epoch 25/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0041    \n",
      "Epoch 26/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0040    \n",
      "Epoch 27/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0040    \n",
      "Epoch 28/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0039    \n",
      "Epoch 29/50\n",
      "200000/200000 [==============================] - 25s - loss: 0.0038    \n",
      "Epoch 30/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0038    \n",
      "Epoch 31/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0037    \n",
      "Epoch 32/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0037    \n",
      "Epoch 33/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0036    \n",
      "Epoch 34/50\n",
      "200000/200000 [==============================] - 23s - loss: 0.0036    \n",
      "Epoch 35/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0035    \n",
      "Epoch 36/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0035    \n",
      "Epoch 37/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0035    \n",
      "Epoch 38/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0034    \n",
      "Epoch 39/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0034    \n",
      "Epoch 40/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0034    \n",
      "Epoch 41/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0034    \n",
      "Epoch 42/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0033    \n",
      "Epoch 43/50\n",
      "200000/200000 [==============================] - 24s - loss: 0.0033    \n",
      "Epoch 44/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0033    \n",
      "Epoch 45/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0032    \n",
      "Epoch 46/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0032    \n",
      "Epoch 47/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0032    \n",
      "Epoch 48/50\n",
      "200000/200000 [==============================] - 21s - loss: 0.0032    \n",
      "Epoch 49/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0032    \n",
      "Epoch 50/50\n",
      "200000/200000 [==============================] - 22s - loss: 0.0032    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1261ff990>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = expert_data[\"observations\"], y = np.squeeze(expert_data[\"actions\"]), nb_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"BehaviorCloning_Walker2d.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"BehaviorCloning_Humanoid.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('expert_policy_file', type=str)\n",
    "parser.add_argument('envname', type=str)\n",
    "parser.add_argument('--render', action='store_true')\n",
    "parser.add_argument(\"--max_timesteps\", type=int)\n",
    "parser.add_argument('--num_rollouts', type=int, default=20,\n",
    "                    help='Number of expert roll outs')\n",
    "#args = parser.parse_args([\"experts/Humanoid-v1.pkl\", \"Humanoid-v1\", \"--render\", \"--num_rollouts\", \"200\"])\n",
    "args = parser.parse_args([\"experts/Humanoid-v1.pkl\", \"Humanoid-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Reacher-v1.pkl\", \"Reacher-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Reacher-v1.pkl\", \"Reacher-v1\", \"--num_rollouts\", \"20000\"])\n",
    "#args = parser.parse_args([\"experts/Hopper-v1.pkl\", \"Hopper-v1\", \"--render\", \"--num_rollouts\", \"20\"])\n",
    "#args = parser.parse_args([\"experts/Walker2d-v1.pkl\", \"Walker2d-v1\", \"--num_rollouts\", \"20\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-26 21:27:35,155] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded and built\n",
      "('The observation space for this environment:', Box(376,))\n",
      "('The action space for this environment:', Box(17,))\n",
      "('iter', 0)\n",
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n",
      "1000/1000\n",
      "('iter', 1)\n",
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-c1ca692da392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# done: boolean, whether it's time to reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# _, info: dict, diagnostic information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the simulator perform the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtotalr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/envs/mujoco/humanoid.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpos_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmass_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpos_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmass_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0malive_bonus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/gym/envs/mujoco/mujoco_env.pyc\u001b[0m in \u001b[0;36mdo_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/liujxing/anaconda/lib/python2.7/site-packages/mujoco_py/mjcore.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mmjlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmj_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('loaded and built')\n",
    "\n",
    "import gym\n",
    "env = gym.make(args.envname) # generate the environment\n",
    "# This indicates the observation space, i.e. the universe in this environment, is indicated by this 376 dimension real rector\n",
    "print(\"The observation space for this environment:\", env.observation_space)\n",
    "# This indicates the action space, i.e. the action in this environment. Here the action is indicated by 17 dimension real vector.\n",
    "print(\"The action space for this environment:\", env.action_space)\n",
    "# use args.max_timesteps if it is not None, else use envs.spec.timestep_limit, here is envs.spec.timestep_limit = 1000\n",
    "max_steps = args.max_timesteps or env.spec.timestep_limit \n",
    "\n",
    "returns = []\n",
    "observations = []\n",
    "actions = []\n",
    "for i in range(args.num_rollouts):\n",
    "    print('iter', i)\n",
    "    obs = env.reset() # obs is not the initial observation of the environment\n",
    "    done = False\n",
    "    totalr = 0. # total reward\n",
    "    steps = 0\n",
    "    while True:\n",
    "    #while not done:\n",
    "        # the obs[None,:] is equivalent to obs.reshape(1,-1)\n",
    "        action = model.predict(obs[None,:]) # generate the action from the current observation\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        # obs: object, an environment-specific object representing the observation of the environment\n",
    "        # r: float, reward obtained from action\n",
    "        # done: boolean, whether it's time to reset the environment\n",
    "        # _, info: dict, diagnostic information\n",
    "        obs, r, done, _ = env.step(action) # the simulator perform the action\n",
    "        totalr += r\n",
    "        steps += 1\n",
    "        if args.render:\n",
    "            env.render()\n",
    "        if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    returns.append(totalr)\n",
    "\n",
    "print('returns', returns) # the sum of rewards acorss each rollout\n",
    "print('mean return', np.mean(returns)) # the mean across each rollout\n",
    "print('std of return', np.std(returns)) # the std across each rollout\n",
    "\n",
    "expert_data = {'observations': np.array(observations), # obsevations array has shape rollout * max_steps, each observation has length 376 as dimension of world\n",
    "               'actions': np.array(actions)} # \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
