{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from dqn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\", \"lr_schedule\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LinearSchedule: output the exploration probability which varies linearly with the number of already performed time steps from initial probability (1.0) to final probability (0.1)\n",
    "- env: the Atari environment, obtained by gym.make(\"Pong-v0\")\n",
    "- optimizer_spec: the namedtuple from OptimizerSpec, has field \"constructor\", \"kwargs\", \"lr_schedule\"\n",
    "- session: the tf session\n",
    "- exploration: a schedule which outputs the probability of exploration\n",
    "- stopping_criterion: take a function which uses (env, t) as input and return boolean indicating whether the RL should stop.\n",
    "- learning_starts: the number of environment steps before starting the learning algorithm\n",
    "- learning_freq: the number of environment steps to take between every experience replay\n",
    "- frame_history_len: the number of frame to use as input to the model\n",
    "- target_update_freq: the number of environment steps before the target network is updated\n",
    "- grad_norm_clipping: clip the gradient to this value to prevent extremely large gradient\n",
    "- q_func: takes a tf tensor representing the input image, and the number of actions, and the scope which all variables are created, and a boolean of whether previously created variables should be reused, then output the q-function for all actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-17 00:13:14,570] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the q_func\n",
    "import tensorflow.contrib.layers as layers\n",
    "def atari_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=512,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "q_func = atari_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the tf session\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "    tf_config = tf.ConfigProto(\n",
    "        inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf_config)\n",
    "    #print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    return session\n",
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "num_timesteps = int(1e4)\n",
    "num_iterations = float(num_timesteps) / 4.0\n",
    "lr_multiplier = 1.0\n",
    "lr_schedule = PiecewiseSchedule([\n",
    "                                     (0,                   1e-4 * lr_multiplier),\n",
    "                                     (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                     (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                ],\n",
    "                                outside_value=5e-5 * lr_multiplier)\n",
    "optimizer_spec = OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the rest of the input parameters\n",
    "exploration=LinearSchedule(1000000, 0.1)\n",
    "stopping_criterion=None\n",
    "replay_buffer_size=1000000\n",
    "batch_size=32\n",
    "gamma=0.99\n",
    "learning_starts=50000\n",
    "learning_freq=4\n",
    "frame_history_len=4\n",
    "target_update_freq=10000\n",
    "grad_norm_clipping=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(env,\n",
    "          q_func,\n",
    "          optimizer_spec,\n",
    "          session,\n",
    "          exploration=LinearSchedule(1000000, 0.1),\n",
    "          stopping_criterion=None,\n",
    "          replay_buffer_size=1000000,\n",
    "          batch_size=32,\n",
    "          gamma=0.99,\n",
    "          learning_starts=50000,\n",
    "          learning_freq=4,\n",
    "          frame_history_len=4,\n",
    "          target_update_freq=10000,\n",
    "          grad_norm_clipping=10):\n",
    "    \"\"\"Run Deep Q-learning algorithm.\n",
    "\n",
    "    You can specify your own convnet using q_func.\n",
    "\n",
    "    All schedules are w.r.t. total number of steps taken in the environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.Env\n",
    "        gym environment to train on.\n",
    "    q_func: function\n",
    "        Model to use for computing the q function. It should accept the\n",
    "        following named arguments:\n",
    "            img_in: tf.Tensor\n",
    "                tensorflow tensor representing the input image\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "            scope: str\n",
    "                scope in which all the model related variables\n",
    "                should be created\n",
    "            reuse: bool\n",
    "                whether previously created variables should be reused.\n",
    "    optimizer_spec: OptimizerSpec\n",
    "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "        for the optimizer\n",
    "    session: tf.Session\n",
    "        tensorflow session to use.\n",
    "    exploration: rl_algs.deepq.utils.schedules.Schedule\n",
    "        schedule for probability of chosing random action.\n",
    "    stopping_criterion: (env, t) -> bool\n",
    "        should return true when it's ok for the RL algorithm to stop.\n",
    "        takes in env and the number of steps executed so far.\n",
    "    replay_buffer_size: int\n",
    "        How many memories to store in the replay buffer.\n",
    "    batch_size: int\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma: float\n",
    "        Discount Factor\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    frame_history_len: int\n",
    "        How many past frames to include as input to the model.\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    grad_norm_clipping: float or None\n",
    "        If not None gradients' norms are clipped to this value.\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.Box\n",
    "    assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "    ###############\n",
    "    # BUILD MODEL #\n",
    "    ###############\n",
    "\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "        input_shape = env.observation_space.shape\n",
    "    else:\n",
    "        img_h, img_w, img_c = env.observation_space.shape\n",
    "        input_shape = (img_h, img_w, frame_history_len * img_c)\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # set up placeholders\n",
    "    # placeholder for current observation (or state)\n",
    "    obs_t_ph              = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "    # placeholder for current action\n",
    "    act_t_ph              = tf.placeholder(tf.int32,   [None])\n",
    "    # placeholder for current reward\n",
    "    rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "    # placeholder for next observation (or state)\n",
    "    obs_tp1_ph            = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "    # placeholder for end of episode mask\n",
    "    # this value is 1 if the next state corresponds to the end of an episode,\n",
    "    # in which case there is no Q-value at the next state; at the end of an\n",
    "    # episode, only the current state reward contributes to the target, not the\n",
    "    # next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "    done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "    # casting to float on GPU ensures lower data transfer times.\n",
    "    obs_t_float   = tf.cast(obs_t_ph,   tf.float32) / 255.0\n",
    "    obs_tp1_float = tf.cast(obs_tp1_ph, tf.float32) / 255.0\n",
    "\n",
    "    # Here, you should fill in your own code to compute the Bellman error. This requires\n",
    "    # evaluating the current and next Q-values and constructing the corresponding error.\n",
    "    # TensorFlow will differentiate this error for you, you just need to pass it to the\n",
    "    # optimizer. See assignment text for details.\n",
    "    # Your code should produce one scalar-valued tensor: total_error\n",
    "    # This will be passed to the optimizer in the provided code below.\n",
    "    # Your code should also produce two collections of variables:\n",
    "    # q_func_vars\n",
    "    # target_q_func_vars\n",
    "    # These should hold all of the variables of the Q-function network and target network,\n",
    "    # respectively. A convenient way to get these is to make use of TF's \"scope\" feature.\n",
    "    # For example, you can create your Q-function network with the scope \"q_func\" like this:\n",
    "    # <something> = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "    # And then you can obtain the variables like this:\n",
    "    # q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "    # Older versions of TensorFlow may require using \"VARIABLES\" instead of \"GLOBAL_VARIABLES\"\n",
    "    ######\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # the Q-value from Q-value network of current state of all actions\n",
    "    q_t = q_func(obs_t_float, num_actions, \"q_func\", False)\n",
    "    # the Q-value from Q-value network of current state of selected action\n",
    "    N = tf.shape(act_t_ph)[0]\n",
    "    ind = tf.transpose(tf.stack([tf.range(start=0, limit=N, delta=1), act_t_ph]))\n",
    "    q_t = tf.gather_nd(q_t, ind)\n",
    "\n",
    "    # the Q-value from target network from next state\n",
    "    q_target_tp1 = q_func(obs_tp1_float, num_actions, \"target_q_func\", False)\n",
    "    # the backed-up Q-value from target network of current state\n",
    "    q_target = rew_t_ph + tf.reduce_max(q_target_tp1, 1, True) * gamma * done_mask_ph\n",
    "\n",
    "    # get the total error from the mismatch between the Q-value and backed-up Q-value from target network\n",
    "    total_error = tf.reduce_sum((q_target - q_t)**2)\n",
    "\n",
    "    # get the variables from Q-value and target networks\n",
    "    q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "    target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"target_q_func\")\n",
    "\n",
    "    ######\n",
    "\n",
    "    # construct optimization op (with gradient clipping)\n",
    "    learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "    optimizer = optimizer_spec.constructor(learning_rate=learning_rate, **optimizer_spec.kwargs)\n",
    "    train_fn = minimize_and_clip(optimizer, total_error,\n",
    "                 var_list=q_func_vars, clip_val=grad_norm_clipping)\n",
    "\n",
    "    # update_target_fn will be called periodically to copy Q network to target Q network\n",
    "    update_target_fn = []\n",
    "    for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                               sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "        update_target_fn.append(var_target.assign(var))\n",
    "    update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "    # construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "    ###############\n",
    "    # RUN ENV     #\n",
    "    ###############\n",
    "    model_initialized = False\n",
    "    num_param_updates = 0\n",
    "    mean_episode_reward      = -float('nan')\n",
    "    best_mean_episode_reward = -float('inf')\n",
    "    last_obs = env.reset()\n",
    "    LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "    for t in itertools.count():\n",
    "        ### 1. Check stopping criterion\n",
    "        if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "            break\n",
    "\n",
    "        ### 2. Step the env and store the transition\n",
    "        # At this point, \"last_obs\" contains the latest observation that was\n",
    "        # recorded from the simulator. Here, your code needs to store this\n",
    "        # observation and its outcome (reward, next observation, etc.) into\n",
    "        # the replay buffer while stepping the simulator forward one step.\n",
    "        # At the end of this block of code, the simulator should have been\n",
    "        # advanced one step, and the replay buffer should contain one more\n",
    "        # transition.\n",
    "        # Specifically, last_obs must point to the new latest observation.\n",
    "        # Useful functions you'll need to call:\n",
    "        # obs, reward, done, info = env.step(action)\n",
    "        # this steps the environment forward one step\n",
    "        # obs = env.reset()\n",
    "        # this resets the environment if you reached an episode boundary.\n",
    "        # Don't forget to call env.reset() to get a new observation if done\n",
    "        # is true!!\n",
    "        # Note that you cannot use \"last_obs\" directly as input\n",
    "        # into your network, since it needs to be processed to include context\n",
    "        # from previous frames. You should check out the replay buffer\n",
    "        # implementation in dqn_utils.py to see what functionality the replay\n",
    "        # buffer exposes. The replay buffer has a function called\n",
    "        # encode_recent_observation that will take the latest observation\n",
    "        # that you pushed into the buffer and compute the corresponding\n",
    "        # input that should be given to a Q network by appending some\n",
    "        # previous frames.\n",
    "        # Don't forget to include epsilon greedy exploration!\n",
    "        # And remember that the first time you enter this loop, the model\n",
    "        # may not yet have been initialized (but of course, the first step\n",
    "        # might as well be random, since you haven't trained your net...)\n",
    "\n",
    "        #####\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        index_frame = replay_buffer.store_frame(last_obs)\n",
    "    \n",
    "        explorationProbability = exploration.value(t) # get exploration probability\n",
    "        if (random.random() < explorationProbability or not model_initialized):\n",
    "            # randomly explore the space\n",
    "            action = random.randint(0, num_actions - 1)\n",
    "        else:\n",
    "            # follow the action from maximum Q-value action\n",
    "            inputTensor = tf.placeholder(tf.uint8, list(input_shape))\n",
    "            inputTensor_float = tf.expand_dims(tf.cast(inputTensor, tf.float32) / 255.0, 0)\n",
    "            recent_obs = replay_buffer.encode_recent_observation()\n",
    "            q_val = session.run(q_func(inputTensor_float, num_actions, \"q_func\", True), feed_dict={inputTensor:recent_obs})\n",
    "            action = np.argmax(q_val)\n",
    "\n",
    "        # perform the action for one time step\n",
    "        last_obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            last_obs = env.reset()\n",
    "\n",
    "        # store the rewards into the replay buffers\n",
    "        replay_buffer.store_effect(index_frame, action, reward, done)\n",
    "\n",
    "        #####\n",
    "\n",
    "        # at this point, the environment should have been advanced one step (and\n",
    "        # reset if done was true), and last_obs should point to the new latest\n",
    "        # observation\n",
    "\n",
    "        ### 3. Perform experience replay and train the network.\n",
    "        # note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "        if (t > learning_starts and\n",
    "                t % learning_freq == 0 and\n",
    "                replay_buffer.can_sample(batch_size)):\n",
    "            # Here, you should perform training. Training consists of four steps:\n",
    "            # 3.a: use the replay buffer to sample a batch of transitions (see the\n",
    "            # replay buffer code for function definition, each batch that you sample\n",
    "            # should consist of current observations, current actions, rewards,\n",
    "            # next observations, and done indicator).\n",
    "            # 3.b: initialize the model if it has not been initialized yet; to do\n",
    "            # that, call\n",
    "            #    initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "            #        obs_t_ph: obs_t_batch,\n",
    "            #        obs_tp1_ph: obs_tp1_batch,\n",
    "            #    })\n",
    "            # where obs_t_batch and obs_tp1_batch are the batches of observations at\n",
    "            # the current and next time step. The boolean variable model_initialized\n",
    "            # indicates whether or not the model has been initialized.\n",
    "            # Remember that you have to update the target network too (see 3.d)!\n",
    "            # 3.c: train the model. To do this, you'll need to use the train_fn and\n",
    "            # total_error ops that were created earlier: total_error is what you\n",
    "            # created to compute the total Bellman error in a batch, and train_fn\n",
    "            # will actually perform a gradient step and update the network parameters\n",
    "            # to reduce total_error. When calling session.run on these you'll need to\n",
    "            # populate the following placeholders:\n",
    "            # obs_t_ph\n",
    "            # act_t_ph\n",
    "            # rew_t_ph\n",
    "            # obs_tp1_ph\n",
    "            # done_mask_ph\n",
    "            # (this is needed for computing total_error)\n",
    "            # learning_rate -- you can get this from optimizer_spec.lr_schedule.value(t)\n",
    "            # (this is needed by the optimizer to choose the learning rate)\n",
    "            # 3.d: periodically update the target network by calling\n",
    "            # session.run(update_target_fn)\n",
    "            # you should update every target_update_freq steps, and you may find the\n",
    "            # variable num_param_updates useful for this (it was initialized to 0)\n",
    "            #####\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # get the observations, actions, rewards, next observations, and done mask from replay buffer\n",
    "            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "            # initialize the network if they are not initialized\n",
    "            if not model_initialized:\n",
    "                initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "                    obs_t_ph: obs_batch,\n",
    "                    obs_tp1_ph: next_obs_batch,\n",
    "                })\n",
    "                model_initialized = True\n",
    "            # train the model\n",
    "            session.run(train_fn, {obs_t_ph:obs_batch,\n",
    "                                   act_t_ph:act_batch,\n",
    "                                   rew_t_ph:rew_batch,\n",
    "                                   obs_tp1_ph:next_obs_batch,\n",
    "                                   done_mask_ph:done_mask,\n",
    "                                   learning_rate:optimizer_spec.lr_schedule.value(t)\n",
    "                                   })\n",
    "            num_param_updates += 1\n",
    "            # update the target network\n",
    "            if num_param_updates%target_update_freq == 0:\n",
    "                session.run(update_target_fn)\n",
    "\n",
    "            #####\n",
    "\n",
    "        ### 4. Log progress\n",
    "        episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "        if len(episode_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "        if len(episode_rewards) > 100:\n",
    "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "        if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "            print(\"Timestep %d\" % (t,))\n",
    "            print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "            print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "            print(\"episodes %d\" % len(episode_rewards))\n",
    "            print(\"exploration %f\" % exploration.value(t))\n",
    "            print(\"learning_rate %f\" % optimizer_spec.lr_schedule.value(t))\n",
    "            sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(env.observation_space) == gym.spaces.Box\n",
    "assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "###############\n",
    "# BUILD MODEL #\n",
    "###############\n",
    "\n",
    "if len(env.observation_space.shape) == 1:\n",
    "    # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "    input_shape = env.observation_space.shape\n",
    "else:\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up placeholders\n",
    "# placeholder for current observation (or state)\n",
    "obs_t_ph              = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "# placeholder for current action\n",
    "act_t_ph              = tf.placeholder(tf.int32,   [None])\n",
    "# placeholder for current reward\n",
    "rew_t_ph              = tf.placeholder(tf.float32, [None])\n",
    "# placeholder for next observation (or state)\n",
    "obs_tp1_ph            = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "# placeholder for end of episode mask\n",
    "# this value is 1 if the next state corresponds to the end of an episode,\n",
    "# in which case there is no Q-value at the next state; at the end of an\n",
    "# episode, only the current state reward contributes to the target, not the\n",
    "# next state Q-value (i.e. target is just rew_t_ph, not rew_t_ph + gamma * q_tp1)\n",
    "done_mask_ph          = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# casting to float on GPU ensures lower data transfer times.\n",
    "obs_t_float   = tf.cast(obs_t_ph,   tf.float32) / 255.0\n",
    "obs_tp1_float = tf.cast(obs_tp1_ph, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Here, you should fill in your own code to compute the Bellman error. This requires\n",
    "# evaluating the current and next Q-values and constructing the corresponding error.\n",
    "# TensorFlow will differentiate this error for you, you just need to pass it to the\n",
    "# optimizer. See assignment text for details.\n",
    "# Your code should produce one scalar-valued tensor: total_error\n",
    "# This will be passed to the optimizer in the provided code below.\n",
    "# Your code should also produce two collections of variables:\n",
    "# q_func_vars\n",
    "# target_q_func_vars\n",
    "# These should hold all of the variables of the Q-function network and target network,\n",
    "# respectively. A convenient way to get these is to make use of TF's \"scope\" feature.\n",
    "# For example, you can create your Q-function network with the scope \"q_func\" like this:\n",
    "# <something> = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "# And then you can obtain the variables like this:\n",
    "# q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "# Older versions of TensorFlow may require using \"VARIABLES\" instead of \"GLOBAL_VARIABLES\"\n",
    "######\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# the Q-value from Q-value network of current state of all actions\n",
    "q_t = q_func(obs_t_float, num_actions, \"q_func\", False)\n",
    "# the Q-value from Q-value network of current state of selected action\n",
    "N = tf.shape(act_t_ph)[0]\n",
    "ind = tf.transpose(tf.stack([tf.range(start=0, limit=N, delta=1), act_t_ph]))\n",
    "q_t_action = tf.gather_nd(q_t, ind)\n",
    "\n",
    "# the Q-value from target network from next state\n",
    "q_target_tp1 = q_func(obs_tp1_float, num_actions, \"target_q_func\", False)\n",
    "# the backed-up Q-value from target network of current state\n",
    "q_target = rew_t_ph + tf.reduce_max(q_target_tp1, 1, False) * gamma * done_mask_ph\n",
    "\n",
    "# get the total error from the mismatch between the Q-value and backed-up Q-value from target network\n",
    "total_error = tf.reduce_sum((q_target - q_t_action)**2)\n",
    "\n",
    "# get the variables from Q-value and target networks\n",
    "q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='q_func')\n",
    "target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"target_q_func\")\n",
    "\n",
    "######\n",
    "\n",
    "# construct optimization op (with gradient clipping)\n",
    "learning_rate = tf.placeholder(tf.float32, (), name=\"learning_rate\")\n",
    "optimizer = optimizer_spec.constructor(learning_rate=learning_rate, **optimizer_spec.kwargs)\n",
    "train_fn = minimize_and_clip(optimizer, total_error,\n",
    "             var_list=q_func_vars, clip_val=grad_norm_clipping)\n",
    "\n",
    "# update_target_fn will be called periodically to copy Q network to target Q network\n",
    "update_target_fn = []\n",
    "for var, var_target in zip(sorted(q_func_vars,        key=lambda v: v.name),\n",
    "                           sorted(target_q_func_vars, key=lambda v: v.name)):\n",
    "    update_target_fn.append(var_target.assign(var))\n",
    "update_target_fn = tf.group(*update_target_fn)\n",
    "\n",
    "# construct the replay buffer\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "###############\n",
    "# RUN ENV     #\n",
    "###############\n",
    "model_initialized = False\n",
    "num_param_updates = 0\n",
    "mean_episode_reward      = -float('nan')\n",
    "best_mean_episode_reward = -float('inf')\n",
    "last_obs = env.reset()\n",
    "LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# define the tensor for getting the action with maximum Q-value\n",
    "inputTensor = tf.placeholder(tf.uint8, list(input_shape))\n",
    "inputTensor_float = tf.expand_dims(tf.cast(inputTensor, tf.float32) / 255.0, 0)\n",
    "outputTensor_q_val = tf.gather(q_func(inputTensor_float, num_actions, \"q_func\", True), 0)\n",
    "outputTensor_action = tf.argmax(outputTensor_q_val, 0)\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "for t in itertools.count():\n",
    "    if t%1000 == 0:\n",
    "        print(t)\n",
    "\n",
    "    ### 1. Check stopping criterion\n",
    "    if stopping_criterion is not None and stopping_criterion(env, t):\n",
    "        break\n",
    "\n",
    "    ### 2. Step the env and store the transition\n",
    "    # At this point, \"last_obs\" contains the latest observation that was\n",
    "    # recorded from the simulator. Here, your code needs to store this\n",
    "    # observation and its outcome (reward, next observation, etc.) into\n",
    "    # the replay buffer while stepping the simulator forward one step.\n",
    "    # At the end of this block of code, the simulator should have been\n",
    "    # advanced one step, and the replay buffer should contain one more\n",
    "    # transition.\n",
    "    # Specifically, last_obs must point to the new latest observation.\n",
    "    # Useful functions you'll need to call:\n",
    "    # obs, reward, done, info = env.step(action)\n",
    "    # this steps the environment forward one step\n",
    "    # obs = env.reset()\n",
    "    # this resets the environment if you reached an episode boundary.\n",
    "    # Don't forget to call env.reset() to get a new observation if done\n",
    "    # is true!!\n",
    "    # Note that you cannot use \"last_obs\" directly as input\n",
    "    # into your network, since it needs to be processed to include context\n",
    "    # from previous frames. You should check out the replay buffer\n",
    "    # implementation in dqn_utils.py to see what functionality the replay\n",
    "    # buffer exposes. The replay buffer has a function called\n",
    "    # encode_recent_observation that will take the latest observation\n",
    "    # that you pushed into the buffer and compute the corresponding\n",
    "    # input that should be given to a Q network by appending some\n",
    "    # previous frames.\n",
    "    # Don't forget to include epsilon greedy exploration!\n",
    "    # And remember that the first time you enter this loop, the model\n",
    "    # may not yet have been initialized (but of course, the first step\n",
    "    # might as well be random, since you haven't trained your net...)\n",
    "\n",
    "    #####\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    index_frame = replay_buffer.store_frame(last_obs)\n",
    "\n",
    "    explorationProbability = exploration.value(t) # get exploration probability\n",
    "    if (random.random() < explorationProbability or not model_initialized):\n",
    "        # randomly explore the space\n",
    "        action = random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        # follow the action from maximum Q-value action\n",
    "        recent_obs = np.expand_dims(replay_buffer.encode_recent_observation(), 0)\n",
    "        action = np.argmax(session.run(q_t, feed_dict={obs_t_ph:recent_obs}))\n",
    "\n",
    "\n",
    "\n",
    "    # perform the action for one time step\n",
    "    last_obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        last_obs = env.reset()\n",
    "\n",
    "    # store the rewards into the replay buffers\n",
    "    replay_buffer.store_effect(index_frame, action, reward, done)\n",
    "\n",
    "    #####\n",
    "\n",
    "    # at this point, the environment should have been advanced one step (and\n",
    "    # reset if done was true), and last_obs should point to the new latest\n",
    "    # observation\n",
    "\n",
    "    ### 3. Perform experience replay and train the network.\n",
    "    # note that this is only done if the replay buffer contains enough samples\n",
    "    # for us to learn something useful -- until then, the model will not be\n",
    "    # initialized and random actions should be taken\n",
    "    if (t > learning_starts and\n",
    "            t % learning_freq == 0 and\n",
    "            replay_buffer.can_sample(batch_size)):\n",
    "        # Here, you should perform training. Training consists of four steps:\n",
    "        # 3.a: use the replay buffer to sample a batch of transitions (see the\n",
    "        # replay buffer code for function definition, each batch that you sample\n",
    "        # should consist of current observations, current actions, rewards,\n",
    "        # next observations, and done indicator).\n",
    "        # 3.b: initialize the model if it has not been initialized yet; to do\n",
    "        # that, call\n",
    "        #    initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "        #        obs_t_ph: obs_t_batch,\n",
    "        #        obs_tp1_ph: obs_tp1_batch,\n",
    "        #    })\n",
    "        # where obs_t_batch and obs_tp1_batch are the batches of observations at\n",
    "        # the current and next time step. The boolean variable model_initialized\n",
    "        # indicates whether or not the model has been initialized.\n",
    "        # Remember that you have to update the target network too (see 3.d)!\n",
    "        # 3.c: train the model. To do this, you'll need to use the train_fn and\n",
    "        # total_error ops that were created earlier: total_error is what you\n",
    "        # created to compute the total Bellman error in a batch, and train_fn\n",
    "        # will actually perform a gradient step and update the network parameters\n",
    "        # to reduce total_error. When calling session.run on these you'll need to\n",
    "        # populate the following placeholders:\n",
    "        # obs_t_ph\n",
    "        # act_t_ph\n",
    "        # rew_t_ph\n",
    "        # obs_tp1_ph\n",
    "        # done_mask_ph\n",
    "        # (this is needed for computing total_error)\n",
    "        # learning_rate -- you can get this from optimizer_spec.lr_schedule.value(t)\n",
    "        # (this is needed by the optimizer to choose the learning rate)\n",
    "        # 3.d: periodically update the target network by calling\n",
    "        # session.run(update_target_fn)\n",
    "        # you should update every target_update_freq steps, and you may find the\n",
    "        # variable num_param_updates useful for this (it was initialized to 0)\n",
    "        #####\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # get the observations, actions, rewards, next observations, and done mask from replay buffer\n",
    "        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "        # initialize the network if they are not initialized\n",
    "        if not model_initialized:\n",
    "            initialize_interdependent_variables(session, tf.global_variables(), {\n",
    "                obs_t_ph: obs_batch,\n",
    "                obs_tp1_ph: next_obs_batch,\n",
    "            })\n",
    "            session.run(update_target_fn)\n",
    "            model_initialized = True\n",
    "        # train the model\n",
    "        session.run(train_fn, {obs_t_ph:obs_batch,\n",
    "                               act_t_ph:act_batch,\n",
    "                               rew_t_ph:rew_batch,\n",
    "                               obs_tp1_ph:next_obs_batch,\n",
    "                               done_mask_ph:done_mask,\n",
    "                               learning_rate:optimizer_spec.lr_schedule.value(t)\n",
    "                               })\n",
    "        num_param_updates += 1\n",
    "\n",
    "        # update the target network\n",
    "        if num_param_updates%target_update_freq == 0:\n",
    "            session.run(update_target_fn)\n",
    "\n",
    "        #####\n",
    "\n",
    "    ### 4. Log progress\n",
    "    episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "    if len(episode_rewards) > 0:\n",
    "        mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "    if len(episode_rewards) > 100:\n",
    "        best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "    if t % LOG_EVERY_N_STEPS == 0 and model_initialized:\n",
    "        print(\"Timestep %d\" % (t,))\n",
    "        print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "        print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "        print(\"episodes %d\" % len(episode_rewards))\n",
    "        print(\"exploration %f\" % exploration.value(t))\n",
    "        print(\"learning_rate %f\" % optimizer_spec.lr_schedule.value(t))\n",
    "        sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
