{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import logz\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normc_initializer(std=1.0):\n",
    "    \"\"\"\n",
    "    Initialize array with normalized columns. Each column has standard deviation of input std.\n",
    "    \"\"\"\n",
    "    def _initializer(shape, dtype=None, partition_info=None): #pylint: disable=W0613\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def dense(x, size, name, weight_init=None):\n",
    "    \"\"\"\n",
    "    Dense (fully connected) layer\n",
    "    \"\"\"\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=weight_init)\n",
    "    b = tf.get_variable(name + \"/b\", [size], initializer=tf.zeros_initializer())\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "def fancy_slice_2d(X, inds0, inds1):\n",
    "    \"\"\"\n",
    "    Like numpy's X[inds0, inds1]. Here inds0 are the row indices and inds1 are the column indices.\n",
    "    The array selects X[inds0[i], inds1[i]] for i in range(len(inds0))\n",
    "    \"\"\"\n",
    "    inds0 = tf.cast(inds0, tf.int64)\n",
    "    inds1 = tf.cast(inds1, tf.int64)\n",
    "    shape = tf.cast(tf.shape(X), tf.int64)\n",
    "    ncols = shape[1]\n",
    "    Xflat = tf.reshape(X, [-1])\n",
    "    return tf.gather(Xflat, inds0 * ncols + inds1)\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Compute discounted sum of future values\n",
    "    out[i] = in[i] + gamma * in[i+1] + gamma^2 * in[i+2] + ...\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "\n",
    "def explained_variance_1d(ypred,y):\n",
    "    \"\"\"\n",
    "    1 - Var[ypred - y] / var[y]. \n",
    "    https://www.quora.com/What-is-the-meaning-proportion-of-variance-explained-in-linear-regression\n",
    "    \"\"\"\n",
    "    assert y.ndim == 1 and ypred.ndim == 1    \n",
    "    vary = np.var(y)\n",
    "    return np.nan if vary==0 else 1 - np.var(y-ypred)/vary\n",
    "\n",
    "\n",
    "def categorical_sample_logits(logits): \n",
    "    \"\"\"   \n",
    "    Samples (symbolically) from categorical distribution using logits, where logits is a NxK\n",
    "    matrix specifying N categorical distributions with K categories\n",
    "\n",
    "    specifically, exp(logits) / sum( exp(logits), axis=1 ) is the \n",
    "    probabilities of the different classes\n",
    "\n",
    "    Cleverly uses gumbell trick, based on\n",
    "    https://github.com/tensorflow/tensorflow/issues/456\n",
    "    \"\"\"\n",
    "    U = tf.random_uniform(tf.shape(logits))\n",
    "    return tf.argmax(logits - tf.log(-tf.log(U)), dimension=1)\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "class LinearValueFunction(object):\n",
    "    \"\"\"\n",
    "    A class used to calculate state value function from linear function\n",
    "    \"\"\"\n",
    "    coef = None\n",
    "    def fit(self, X, y):\n",
    "        Xp = self.preproc(X)\n",
    "        A = Xp.T.dot(Xp)\n",
    "        nfeats = Xp.shape[1]\n",
    "        A[np.arange(nfeats), np.arange(nfeats)] += 1e-3 # a little ridge regression by adding a small value to the diagonal\n",
    "        b = Xp.T.dot(y)\n",
    "        self.coef = np.linalg.solve(A, b) # solve the linear regression slope\n",
    "    def predict(self, X):\n",
    "        if self.coef is None:\n",
    "            return np.zeros(X.shape[0])\n",
    "        else:\n",
    "            return self.preproc(X).dot(self.coef)\n",
    "    def preproc(self, X):\n",
    "        # transform X by including features of X**0, X**1, X**2\n",
    "        return np.concatenate([np.ones([X.shape[0], 1]), X, np.square(X)/2.0], axis=1)\n",
    "    \n",
    "class NnValueFunction(object):\n",
    "    \"\"\"\n",
    "    A class used to calculate state value function from neural network\n",
    "    \"\"\"\n",
    "    pass # YOUR CODE HERE\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    \"\"\"\n",
    "    Leaky Relu activation function\n",
    "    \"\"\"\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * abs(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**main_cartpole**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter=100\n",
    "gamma=1.0\n",
    "min_timesteps_per_batch=1000\n",
    "stepsize=1e-2\n",
    "animate=True\n",
    "logdir=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-23 11:16:42,131] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configure_output_dir: not storing the git diff, probably because you're not in a git repo\n",
      "\u001b[32;1mLogging data to /tmp/experiments/1492960602/log.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "logz.configure_output_dir(logdir)\n",
    "vf = LinearValueFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "# that are computed later in these function\n",
    "# Here a simple feed-forward neural network is defined, using the observation of state as input, and output the probability\n",
    "# of taking each action in the discrete action space\n",
    "sy_ob_no = tf.placeholder(shape=[None, ob_dim], name=\"ob\", dtype=tf.float32) # batch of observations\n",
    "sy_ac_n = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32) # batch of actions taken by the policy, used for policy gradient computation\n",
    "sy_adv_n = tf.placeholder(shape=[None], name=\"adv\", dtype=tf.float32) # advantage function estimate\n",
    "sy_h1 = lrelu(dense(sy_ob_no, 32, \"h1\", weight_init=normc_initializer(1.0))) # hidden layer\n",
    "sy_logits_na = dense(sy_h1, num_actions, \"final\", weight_init=normc_initializer(0.05)) # \"logits\", describing probability distribution of final layer\n",
    "# we use a small initialization for the last layer, so the initial policy has maximal entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here the actions are sampled according to the calculated probability of different actions, and their probability are recorded\n",
    "sy_oldlogits_na = tf.placeholder(shape=[None, num_actions], name='oldlogits', dtype=tf.float32) # logits BEFORE update (just used for KL diagnostic)\n",
    "sy_logp_na = tf.nn.log_softmax(sy_logits_na) # logprobability of actions\n",
    "sy_sampled_ac = categorical_sample_logits(sy_logits_na)[0] # sampled actions, used for defining the policy (NOT computing the policy gradient)\n",
    "sy_n = tf.shape(sy_ob_no)[0]\n",
    "sy_logprob_n = fancy_slice_2d(sy_logp_na, tf.range(sy_n), sy_ac_n) # log-prob of actions taken -- used for policy gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following quantities are just used for computing KL and entropy, JUST FOR DIAGNOSTIC PURPOSES >>>>\n",
    "sy_oldlogp_na = tf.nn.log_softmax(sy_oldlogits_na)\n",
    "sy_oldp_na = tf.exp(sy_oldlogp_na) \n",
    "sy_kl = tf.reduce_sum(sy_oldp_na * (sy_oldlogp_na - sy_logp_na)) / tf.to_float(sy_n)\n",
    "sy_p_na = tf.exp(sy_logp_na)\n",
    "sy_ent = tf.reduce_sum( - sy_p_na * sy_logp_na) / tf.to_float(sy_n)\n",
    "# <<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liujxing/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Here the loss function is defined by the expected value of advantage function times the log probability of the corresponding actions\n",
    "sy_surr = - tf.reduce_mean(sy_adv_n * sy_logprob_n) # Loss function that we'll differentiate to get the policy gradient (\"surr\" is for \"surrogate loss\")\n",
    "sy_stepsize = tf.placeholder(shape=[], dtype=tf.float32) # Symbolic, in case you want to change the stepsize during optimization. (We're not doing that currently)\n",
    "update_op = tf.train.AdamOptimizer(sy_stepsize).minimize(sy_surr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1) \n",
    "# use single thread. on such a small problem, multithreading gives you a slowdown\n",
    "# this way, we can better use multiple cores for different experiments\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.__enter__() # equivalent to `with sess:`\n",
    "tf.global_variables_initializer().run() #pylint: disable=E1101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 ************\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(\"********** Iteration %i ************\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect paths until we have enough timesteps\n",
    "timesteps_this_batch = 0\n",
    "paths = []\n",
    "total_timesteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we keep the total time steps across all paths in this batch above threshold.\n",
    "\n",
    "while True:\n",
    "    ob = env.reset()\n",
    "    terminated = False\n",
    "    obs, acs, rewards = [], [], []\n",
    "    animate_this_episode=(len(paths)==0 and (i % 10 == 0) and animate) # only render the episode for the first path in every 10 episodes\n",
    "    while True: # each iteration creates a path until the path is ended by the environment\n",
    "        if animate_this_episode: \n",
    "            env.render()\n",
    "        obs.append(ob)\n",
    "        ac = sess.run(sy_sampled_ac, feed_dict={sy_ob_no : ob[None]})\n",
    "        acs.append(ac)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        rewards.append(rew)\n",
    "        if done:\n",
    "            break                    \n",
    "    # create the path from the accumulated path\n",
    "    path = {\"observation\" : np.array(obs), \"terminated\" : terminated,\n",
    "            \"reward\" : np.array(rewards), \"action\" : np.array(acs)}\n",
    "    # save the paths to a list\n",
    "    paths.append(path)\n",
    "    # check if the total number of time steps reaches the threshold\n",
    "    timesteps_this_batch += pathlength(path)\n",
    "    if timesteps_this_batch > min_timesteps_per_batch:\n",
    "        break\n",
    "total_timesteps += timesteps_this_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimate advantage function\n",
    "vtargs, vpreds, advs = [], [], []\n",
    "for path in paths:\n",
    "    rew_t = path[\"reward\"]\n",
    "    return_t = discount(rew_t, gamma) # return_t is the discounted sum of future reward from given state-action pair\n",
    "    vpred_t = vf.predict(path[\"observation\"]) # at the first iteration, the value function prediction is 0, and later is from linear regression of value function to observation\n",
    "    adv_t = return_t - vpred_t # advantage function as future return - value function\n",
    "    advs.append(adv_t) # list of advantage function\n",
    "    vtargs.append(return_t) # list of discounted sum of future value\n",
    "    vpreds.append(vpred_t) # list of predicted value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build arrays for policy update\n",
    "ob_no = np.concatenate([path[\"observation\"] for path in paths]) # all observations\n",
    "ac_n = np.concatenate([path[\"action\"] for path in paths]) # all actions\n",
    "adv_n = np.concatenate(advs) # all advantage functions\n",
    "standardized_adv_n = (adv_n - adv_n.mean()) / (adv_n.std() + 1e-8) # all standarized advantage functions\n",
    "vtarg_n = np.concatenate(vtargs) # all sum of future value\n",
    "vpred_n = np.concatenate(vpreds) # all predicted value function\n",
    "vf.fit(ob_no, vtarg_n) # fit the observation to the sum of future value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Policy update by minimizing the loss function\n",
    "_, oldlogits_na = sess.run([update_op, sy_logits_na], feed_dict={sy_ob_no:ob_no, sy_ac_n:ac_n, sy_adv_n:standardized_adv_n, sy_stepsize:stepsize})\n",
    "kl, ent = sess.run([sy_kl, sy_ent], feed_dict={sy_ob_no:ob_no, sy_oldlogits_na:oldlogits_na})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "|       EpRewMean |            23.5 |\n",
      "|       EpLenMean |            23.5 |\n",
      "|        KLOldNew |         0.00197 |\n",
      "|         Entropy |           0.691 |\n",
      "|        EVBefore |               0 |\n",
      "|         EVAfter |           0.271 |\n",
      "|  TimestepsSoFar |        1.01e+03 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Log diagnostics\n",
    "logz.log_tabular(\"EpRewMean\", np.mean([path[\"reward\"].sum() for path in paths]))\n",
    "logz.log_tabular(\"EpLenMean\", np.mean([pathlength(path) for path in paths]))\n",
    "logz.log_tabular(\"KLOldNew\", kl)\n",
    "logz.log_tabular(\"Entropy\", ent)\n",
    "logz.log_tabular(\"EVBefore\", explained_variance_1d(vpred_n, vtarg_n))\n",
    "logz.log_tabular(\"EVAfter\", explained_variance_1d(vf.predict(ob_no), vtarg_n))\n",
    "logz.log_tabular(\"TimestepsSoFar\", total_timesteps)\n",
    "# If you're overfitting, EVAfter will be way larger than EVBefore.\n",
    "# Note that we fit value function AFTER using it to compute the advantage function to avoid introducing bias\n",
    "logz.dump_tabular()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
